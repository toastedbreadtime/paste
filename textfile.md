5. Software Instrumentation &
Statistical Debugging
COSC 3P95:
Software Testing & Analysis
Winter 2023
1
Motivation
● Bugs will escape in-house testing and analysis tools ○ Dynamic analysis (i.e., testing) is unsound
○ Static analysis is incomplete
○ There are limited resources (time, money, people)
● Software ships with unknown (and even known) bugs
2
Statistical Debugging
● Example: We have a program that sometimes passes and
sometimes fails. ○ This outcome can be correlated with events that precede it
–
properties of the input, properties of the execution, properties
of the program state.
○ For instance, if we can find that "the program always fails when
Line 123 is executed, and it always passes when Line 123
is not executed", then we have a strong correlation between Line
123 being executed and failure.
● Correlations make excellent hints as it comes to search for
failure causes
3
An Idea: Statistical Debugging
● Monitor deployed code ○ Online: Collect information from user runs
○ Offline: Analyze information to find bugs
● Effectively a “black box” for software
4
Benefits of Statistical Debugging
Actual runs are a vast resource!
● Crowdsource-based testing ○ Number of real runs >> number of testing runs
● Reality-directed debugging ○ Real-world runs are the ones that matter most
5
Statistical Debugging
● Observe deployed software in the hands of real
end users
● Build statistical models of success & failure
● Guide programmers to the root causes of bugs
● Make software suck less
6
Two Key Questions
• How do we get the data?
• Instrumentation
• What do we do with it?
7
Software Instrumentation
8
Software Instrumentation
● Data collection is the process of preparing and collecting
data. Its purpose is to obtain information to keep on record,
to make decisions about important issues, and to pass
information on to others.
— Wikipedia
● Software instrumentation is the primary method for collecting
data in software systems
● Data collected while instrumenting a system can be used to
analyze faults, performance issues, understand system
behavior, etc.
9
Data Storage
● When you collect data, you must store it somewhere. ○ In memory storage
○ Local Disk
○ Network/Remote Storage
10
INTRUSIVE INSTRUMENTATION
● Intrusive instrumentation is the process of modifying the original
source code by inserting chunks of source code to collect data for
analytical purposes
● Best supported by logging frameworks ○ Java Logging Frameworks ■ log4j [logging.apache.org/log4j]
■ Simple Logging Facade 4 Java (SLF4J) [www.slf4j.org]
○ C++ Logging Frameworks ■ log4cplus [sourceforge.net/projects/log4cplus]
■ Log4cxx [logging.apache.org/log4cxx]
○ C# Logging Frameworks ■ log4net (logging.apache.org/log4net]-
■ nlog [nlog-project.org]
11
Example
12
Unguarded Implementation: is when you insert instrumentation code directly into source
code without any restrictions/constraints on when/how it is executed.
Example: Guarded Instrumentation
● instrumentation code directly into source code, but place
restrictions/constraints on when/how it is executed
13
Guard
Proxy Instrumentation
● Is when you pace instrumentation code in a proxy object and proxy
calls real implementation.
14
Aspect-oriented programing
(AOP): programing paradigm to
increase modularity by
separating cross-cutting
concerns
Problems of Intrusive Instrumentation
● You need the source code in order to instrument the system.
● You MUST understand the source code
● requires a LOT of upfront planning to ensure that instrumentation code is
implemented as part of the normal system's implementation
● Requirements define what data needs to be captured
● Design that integrates instrumentation integrates into overall system
● Part of the normal development process, not an afterthought
● Hard to remove/modify when you no longer need the instrumentation
and/or requirements change
15
What to Collect
16
Logs
● Logs are records of events or messages that occur in a software
system. ○ They provide a detailed history of what happened in a system, and are used for debugging,
monitoring, and analysis.
• Application logs: logs generated by the application code to provide information about
the behavior of the system, such as errors, warnings, or status updates. • Example: logs generated by web servers or reverse proxies, providing information about
incoming requests, such as the IP address of the requestor, the requested URL, and the
response status code.
• System logs: logs generated by the operating system or infrastructure components, such
as log messages from system services, system events, or performance metrics.
17
Source-code Instrumentation
18
Metrics
● Metrics are numerical values that describe the performance or
behavior of a software system. ○ They are used to monitor the health and performance of a system, and to
make data-driven decisions about how to improve it.
● Examples: • Application performance metrics: metrics that measure the performance
of the application, such as response time, request rate, or error rate.
• System resource metrics: metrics that measure the utilization of system
resources, such as CPU usage, memory usage, or disk space usage.
• Business metrics: metrics that measure business-related goals, such as
conversion rate, user engagement, or revenue.
19
20
Which requests lead to that
error or that status?
21
Which requests lead to that
error or that status?
Traces provide Context
Trace
● Traces are end-to-end records of a request or transaction in a
software system. ○ They provide a complete picture of the path taken by a request as it
travels through a system, and are used for debugging, performance
analysis, and troubleshooting.
● Examples ○ Event tracing
○ Database tracing
○ Distributed tracing
22
Problems with Instrumentation and data analysis
• Lack of standardization in trace data format: Different tracing solutions may use
different formats to collect and store trace data, making it difficult to compare and
analyze data across different systems.
• Scalability limitations of trace data storage: Storing large volumes of trace data can be
challenging, leading to slow queries and data loss.
• Challenges with Trace Data Analysis and Visualization: Analyzing and visualizing trace
data can be complex, especially for large-scale applications with high-velocity trace data.
• Difficulty in correlating trace data with other data sources: Trace data may not provide
the full picture, especially when it is not correlated with other data sources such as logs,
metrics, and alerts.
• Lack of support for real-time analysis: Traditional tracing solutions may not support realtime analysis, making it difficult to troubleshoot issues in real-time.
• Limited support for distributed tracing: Traditional tracing solutions may not support
distributed tracing, making it difficult to understand the behavior of applications that
run across multiple nodes.
23
Monolith Architecture
● Monolithic software architecture is a traditional approach to software
design where an application is built as a single, tightly coupled unit. In
this architecture, all the components of the application, such as the
user interface, business logic, and data storage, are combined into a
single executable.
● It is relatively simple to develop, test, and deploy, as all the
components are part of a single codebase. Additionally, it can be
easier to understand the overall design of an application, as all the
components are in one place.
24
Monolith Architecture
● monolithic architecture have several disadvantages.
● As the size and complexity of an application grows, it becomes more
difficult to manage and maintain.
● Additionally, making changes to one part of the application can affect
the entire system, making it difficult to deploy updates incrementally.
● Monolithic architecture can also lead to slow performance and
scalability issues as the size of an application grows, as all components
are tightly coupled and share the same resources.
● For these reasons, many modern applications have adopted a
microservice architecture, which breaks down an application into
smaller, independent services that can be developed, tested, and
deployed independently.
25
Microservice Architecture
● Microservice Architecture is a software design pattern that structures
an application as a collection of loosely coupled, independent
services. Each service runs in its own process and communicates with
other services through APIs. This has several benefits over traditional
monolithic architecture:
1. Scalability: Microservices can be deployed, scaled, and managed independently, making it
easier to handle increased loads.
2. Resilience: If one service fails, it does not impact the entire application. The failure of one
service only affects a small portion of the application.
3. Flexibility: Microservices can be developed and deployed using different programming
languages, databases, and other technologies, making it easier to implement new features and
update existing ones.
4. Ease of Deployment: Microservices can be deployed incrementally, making it easier to test and
deploy new features.
5. Improved Modularity: Microservices can be developed and deployed independently, making it
easier to make changes and updates to specific parts of the application.
26
Data Collection in Microservice-based Systems
What to collect data?
How to link them?
How to analyse them?
How to understand them?
How to follow an issue?
Why did it take so long?
Which microservice was responsible?27
Many complex systems must interact to produce even a
single end-to-end response
End-to-end Data Collection
28
System-level tracing
End-to-end tracing
Distributed Tracing
▪ Distributed Tracing is a process of collecting end-to-end transaction
graphs in near real time ▪
A trace represents the entire journey of a request
▪
A span represents single operation call
▪ Distributed Tracing Systems are often used for this purpose. Zipkin is
an example
▪ As a request is flowing from one microservice to another, tracers add
logic to create unique trace Id, span Id
29
Distributed Tracing - Terminology
● Trace: is a tree of spans that follows a request or system from its
source to its destination. ● Each trace is a narrative that tells the requests story as it travels
through the system
30
Distributed Tracing - Terminology
● Span: a logical unit of work in a distributed system ■ The root span describes the end-to-end latency of the entire trace, with child spans representing suboperations
31
Distributed Tracing - Terminology
● Tags & Logs: to annotate the spans with some contextual information ○ Tags apply to the whole span,
○ Logs represent some events that happened during the span execution ● A log always has a timestamp that falls within the span's start-end time interval.
32
Distributed Tracing - Terminology
● Tags (Attributes): Allow you
to search and segment your
data
● Logs (Events): Add
information that is useful
during rot-cause and
debugging
33
Distributed Tracing - Terminology
● To enable tracing and monitoring of the request as it passes through
the system, the context information needs to be propagated from one
component to the next. This is done by including context information
in requests and responses and using it to create a trace that spans
multiple components or services.
● Context propagation is a technique used in distributed systems to
maintain context information as requests are processed across
multiple components or services. The context information can include
information such as trace data, request metadata, authentication
information, and user information.
34
Distributed Tracing - Terminology
● Context Propagation: ○ We need some hints for correlating the spans
to tie together the entire trace.
35
Distributed Tracing Example
36
Spans and Trace
Anatomy of Distributed Tracing
37
Standardization
● Which logging/tracing library do I need to use?
○ Every library has a different format
○ Every languages exposes a different format
● There are different vendors ○ Every vendor has their own format
○ Instrumentation must be decoupled from vendors
● Tracing semantics must not be language dependent.
● Tracing libs in Project X do not handoff to tracing libs in Project Y
38
OpenTracing API
● OpenTracing provides a set of API specification, frameworks
and libraries● Allows developers to add instrumentation to their application code using APIs that
do not lock them into any one particular product or vendor.
39
OpenCensus
● OpenCensus is a set of language-specific libraries for instrumenting an
application, collecting stats (metrics), and exporting data to a
supported backend ○ Google Open-Source community project
40
Example: Python hello-world
41
42
Example: Python hello-world
OpenTelemetry
● An integrated set of APIs and libraries as well as a collection
mechanism via an agent and collector. ○ Problems OpenTelemetry solves: ■ Vendor neutrality for tracing, monitoring and logging ● pre-instrumenting libraries and frameworks to add out-of-the-box and vendor-neutral observability
■ Context Propagation
○ These components are used to generate, collect, and describe telemetry
about distributed systems
43
OpenTelemetry Instrumentation Steps
● Configure Tracer, Create pans (Decorate Spans), Context Propagation
44
Start Span
Decorate
Create Child Span
End Span
Declare Dependencies
Declare Plugins
Create Spans
● tracer.startSpan(name, {parent: , …})
○ This method returns a child of the specified span.
● api.context.with(api.setSpan(api.context.active(), name)) ○ Starts a new span, sets it to be active. Optionally, can get a reference to the span.
● api.trace.getSpan(api.context.active()) ○ Used to access & add information to the current span
● span.addEvent(msg) ○ Adds structured annotations (e.g. "logs") about what is currently happening.
● span.setAttributes(core.Key(key).String(value)...) ○ Adds a structured, typed attribute to the current span. This may include a user id, a
build id, a user-agent, etc.
● span.end() ○ Often used with defer, fires when the unit of work is complete and the span can be
sent
45
SDKs, Exporters, and Collector Services
● OpenTelemetry's SDK implements trace & span creation.
● An exporter can be instantiated to send the data collected by
OpenTelemetry to the backend of your choice. ○ E.g. Jaeger, Lightstep, etc.
● OpenTelemetry collector proxies data between instrumented code
and backend service(s). The exporters can be reconfigured without
changing instrumented code.
46
OpenTelemetry Collector?
● The OTel Collector is an independent binary process, written in Go,
that acts as a ‘universal agent’ for the OpenTelemetry ecosystem.
○ Collect, process, and export telemetry data in a highly performant and
stable manner.
○ Support multiple types of telemetry.
○ Highly extensible and customizable, but with an easy OOB experience.
● Collectors receive, process, and forward telemetry to export
destinations.
● The collector has 4 major components ○ Receivers
○ Processors
○ Exporters
○ Extensions
47
Pipelines
● Telemetry data flows through the collector, from receiver to exporter.
Multiple pipelines in a single collector are configurable, but each must
operate on a single telemetry type - either traces, or metrics.
● All data received by a pipeline will be processed by all processors and
exported to all exporters.
48
Receivers
● Receivers listen on a single port for telemetry data. A single receiver
can send to multiple pipelines.
● Currently, the collector supports the following receivers ○ OTLP
○ OpenCensus
○ Fluentd Forward
○ Jaeger
○ Zipkin
○ Prometheus
○ Host Metrics
○ And third-party/contributed receivers such as ■ Amazon x-ray, Carbon, Redis, Kubernets, …
49
Exporters
● Typically, an exporter will forward all data it receives to a network
endpoint, but can also write to a file or console.
● Similar to receivers, a single exporter may be included in multiple
pipelines.
50
Processors
● Processors are very powerful!
● They run sequentially, and receive all data from the processor before
them. A processor can transform telemetry data, delete it, etc.
● A single processor configuration can apply to multiple pipelines, but
each pipeline gets a unique instance of the processor.
51
Processors Deep Dive
● Receivers, Exporters, and Extensions aren’t super interesting on the
face of things - they receive or emit data, or simply give you some
introspection on the collector instance.
● Processors, however, can do a lot. ○ Sampling
○ Mining
○ Aggregation
○ Span and Attribute Processors
52
Attribute and Span Processors
● Attribute processors can modify the attributes of a span; insert,
update, upsert, delete, and extract based on attribute names or
regular expressions.
● Span processors can modify a span name or attributes based on a span
name
● This can be used for more precise data scrubbing, creating attributes
from span names, hashing attribute fields, and more.
53
OpenTelemetry Architecture
54
OpenTelemetry Architecture Summary
● OpenTelemetry’s SDK implements trace & span
creation.
● A Collector proxies data between instrumented code
and backend service(s) ○ It receives, processes and forwards telemetry to export
destination
○ An exporter can be used to send the data collected to the
backend of your choice. ■ The exporter can be reconfigured without changing the
instrumented code. ● Jeager, Lightstep, etc.
55
Sampling
● Decide not to save all the traces/logs that are created by your
application ○ With the assumption that only some of them are enough to understand
patterns in your code and gain better insights. ■ The case for statistical debugging
● Two main types of sampling: ○ Head-based sampling
○ Tail-based sampling
56
Head-based sampling
● Make the decision to sample or not, upfront, at the beginning of the
trace. ○ This is done at the OTEL distro (configuration) level.
○ Like a percentage of all data/spans to collect
○ Pros: the simplest, most accurate, and most reliable sampling
method which you should prefer over all other methods.
○ Cons: information loss ■ You can’t decide that you want to sample only spans with errors since you do not know this in advance (the
decision to sample happens before the error happened).
57
Tail-based sampling
● Decide at the end of the entire flow, when we already gathered the
data. ○ This type of sampling is done at the OTEL collector (the backend that
receives all the spans) level.
○ Can focus on some specific behaviors ■ Spans with errors
■ Requests longer than a threshold
■
….
58
Comparison
● If you choose to do it at the OTEL distro level (head-based), you
remove redundant data at the source, never needing to worry about it
again. ○ You also minimize data transported in the network.
○ Updating the sampling rate will need a change of the app/service and its
configuration
● Tail-based requires collecting the whole data until a decision can be
made and thus adds overhead.
59
OpenTelemetry Samplers
● OpenTelemetry imlements the actual sampling through “Samplers”.
● “AlwaysOn”sampler ○ It does not sample at all and collects 100% of the spans. ■ In a perfect world, we would use this only, without any cost considerations.
● “AlwaysOff”sampler ○ It samples 0% of the spans! ■ You probably won’t be using this one much, but it could be useful in certain cases such as load testing!
● Parent Based sampler ○ Most popular sampler and is the one recommended by the official
OpenTelemetry documentation ■ When a trace begins we make a decision whether or not to sample a span. Whatever the decision, it is used inside the child spans
● Ratio Based sampler: for example, collect 20% of traces 60
Planning for Instrumentation
61
Design Time Considerations
• Identify the questions that the system needs to answer about its behavior, while in operation • These could be platform level or Business KPI related concerns. • Start with the quality of services that the system needs to provide and trying to find out what
can go wrong and how that can be detected in advance.
• Based on the behavior that requires monitoring, appropriate instrumentation has to be planned.
• Identify the appropriate places to add instrumentation in order to help in structuring the code
well and avoiding refactoring later
• Reduced number of touchpoints but still with maximum coverage.
• Example: tracing all entry and exit points of a component
,
• designing some common code before diverging to handle specific functionalities.
• Pull or pushed based approach: • Push: application generate the instrumentation data as asynchronous events that are externally captured and used
for immediate or delayed processing
• Pull: applications expose APIs about its behavior which can be queried by an external Observability solution.
62
Development Time Considerations
● The developer knows the system better than anyone else. ○ Add sufficient context to the instrumentation data ■ It is important to standardize the context and have them included
consistently across all instrumentation data wherever it is
published.
○ It is better to include the context setting at the framework level so that it
does not depend on manual developer-level discretion.
● Too much instrumentation can become a challenge and can inhibit
performance and overwhelm the analysis. A right balance is required on the
level of instrumentation.
63
Build &Deployment Time Considerations
• Follow ‘Observability as code’ practice. • The practice of using code to automate the creation and management of
observability infrastructure.
• This includes everything from setting up alerts to creating dashboards and visualizations
• This will ensure that they are repeatable and not missed out in any environment or deployment.
64
Operations Considerations
• Make a feedback loop from observations made during operations to
the development team. • The team gets to understand what is causing the difference in performance and
make an informed decision.
• This goes further to influence how developers write code and how
infrastructures are built
• For push-based plan for the capacity to handle the huge volume
collected data. • Appropriate archival and/or sampling, the aggregation has to be made.
• Standard best practices like proactive monitoring and alerting should be in place.
• For pull-based, security mechanisms
65
Non-Intrusive Instrumentation
66
Non-Intrusive Instrumentation
● Non-intrusive instrumentation is the process of collecting data from
software for analytical purposes without modifying the original source
code ○ Dynamic Binary Instrumentation (DBI) is a form of non-intrusive
software instrumentation where instrumentation code is injected
into a binary executable at runtime
○ Monitor both application- and system-level behavior ■ E.g., resource usage, system calls, multi-threading behavior, branching, etc.
67
DBI Pros
● Non-invasive: Instrumentation needs do not have to
be tightly integrated with development process ○ Does not require original source code
● Flexibility: Add/remove instrumentation code from
binary on-demand ○ Dynamically change behavior as needed without
modifying original source code
68
DBI Cons
● Overhead: DBI can introduce significant overhead to a program, as the
insertion of probes can slow down its execution and increase its memory usage. ○ Some DBI tools are virtual machines & can add unwanted overhead
● Complexity: DBI can be complex to implement, especially for large and complex
programs, due to the need to understand the underlying machine code and the
internal structure of the program.
● Compatibility: DBI may not be compatible with all types of programs, especially
those that use advanced features such as dynamic code generation or selfmodifying code.
● Security: DBI can introduce security risks, as the dynamic modification of a
program's machine code can potentially alter its behavior or introduce
vulnerabilities.
69
DBI Tools
● Pin: is a widely-used DBI tool developed by Intel. It is a highly flexible and
customizable tool that supports a wide range of platforms and
architectures. ○ Pin can be used to monitor and collect data about the behavior of programs,
such as their memory access patterns, instruction counts, and system calls.
● DynamoRIO: is an open-source DBI framework that provides a platform for
building custom dynamic tools. It supports a wide range of platforms and
architectures and provides a comprehensive API for building dynamic
tools.
70
DBI Tools
● Valgrind: is a popular open-source DBI tool that can be used for a variety of
purposes, including performance analysis, debugging, and memory leak
detection. It supports a wide range of platforms and architectures and
provides a range of built-in tools for performing various types of analysis.
● FRIDA: is a DBI tool that can be used to inject code into running processes
on Windows, macOS, Linux, iOS, and Android. It provides a Python API for
building custom dynamic tools and supports a wide range of platforms and
architectures.
● QBDI: is an open-source DBI tool that provides a platform for building
dynamic analysis and instrumentation tools. It is designed to be fast and
efficient and supports a wide range of platforms and architectures.
71
Examples of Pintools
● PinTools are specialized tools that can be built using the Pin framework.
Some examples of PinTools include:
● Intel Parallel Studio
— memory debugging, performance analysis,
multithreading correctness analysis and parallelization preparation.
● Intel Software Development Emulator
— enables the development of
applications using instruction set extensions that are not currently
implemented in hardware.
● PinPlay
— capture and replay of the running of multithreaded programs
under pin. Capturing the running of a program helps developers overcome
the non-determinism inherent in multithreading.
72
Summary of Instrumentation
● Intrusive Instrumentation ○ Guarded
○ Non-guarded
○ Proxy-based
● Method of instrumentation: ○ Opentelemetry
● Non-intrusive ○ DBA■ Pintools ● Flexible but expensive (overhead)
73
Statistical Debugging
74
Statistical Debugging
● Statistical debugging is a technique used to identify the root cause of
software bugs by analyzing large amounts of data collected from the
execution of the program.
● Steps: ○ Collect data: opentelemetry/sampling/etc. ■ such as information about the values of variables at different points in the program, the order in which the
variables are processed, and any other relevant information.
○ Analyze the data of (or compare the) successful and unsuccessful
executions ■ Analyze the data and look for patterns that might indicate the cause of the bug ● You might look for variables whose values change unexpectedly, or patterns in the order in which
variables are processed.
○ Identify the bugs: by discovering some facts/correlations/causality
between the data and bug
75
Example 1
76
import java.util.Arrays;
public class SortingProgram {
public static int[] sortNumbers(int[] numbers) {
for (int i = 0; i < numbers.length - 1; i++) {
for (int j = 0; j < numbers.length - 1 - i; j++) {
if (numbers[j] < numbers[j + 1]) {
// Swap the values
int temp = numbers[j];
numbers[j] = numbers[j + 1];
numbers[j + 1] = temp;
}
}
}
return numbers;
}
public static void main(String[] args) {
int[] numbers = {5, 4, 3, 2, 1};
int[] sortedNumbers = sortNumbers(numbers);
System.out.println(Arrays.toString(sortedNumbers)); }
}
The goal is to sort the input numbers
in an ascending order! → The output is // [5, 4, 3, 2, 1]
So, there is a bug!
Example 1
77
import java.util.Arrays;
public class SortingProgram {
public static int[] sortNumbers(int[] numbers) {
for (int i = 0; i < numbers.length - 1; i++) {
for (int j = 0; j < numbers.length - 1 - i; j++) {
if (numbers[j] < numbers[j + 1]) {
// Swap the values
int temp = numbers[j];
numbers[j] = numbers[j + 1];
numbers[j + 1] = temp;
}
}
}
return numbers;
}
public static void main(String[] args) {
int[] numbers = {5, 4, 3, 2, 1};
int[] sortedNumbers = sortNumbers(numbers);
System.out.println(Arrays.toString(sortedNumbers)); }
}
// Bug: should be > instead of < → The output is // [5, 4, 3, 2, 1]
How Statistical Debugging Would help?
● Collect data: You would need to collect data about the program's execution
such as the values of variables at different points in the program, the order
in which the variables are processed, and any other relevant information.
● Analyze the data: You would then analyze the data to look for patterns that
might indicate the cause of the bug. For example, you might look for
variables whose values change unexpectedly, or patterns in the order in
which variables are processed.
● Identify the bug: By analyzing the data, you might discover that the
program is swapping the values of two variables in the wrong order,
causing the numbers to be sorted in descending order instead of ascending
order.
78
Example 2
79
import java.util.Scanner;
public class GradeCalculator {
public static char calculateGrade(double score) {
if (score > 90.0) {
return 'A';
} else if (score > 80.0) {
return 'B';
} else if (score > 70.0) {
return 'C';
} else if (score > 60.0) {
return 'D';
} else {
return 'F';
}
}
public static void main(String[] args) {
Scanner input = new Scanner(System.in);
System.out.print("Enter your score: ");
double score = input.nextDouble();
char grade = calculateGrade(score);
System.out.println("Your grade is: " + grade);
}
}
Is there a bug?
Example 2
80
import java.util.Scanner;
public class GradeCalculator {
public static char calculateGrade(double score) {
if (score > 90.0) {
return 'A';
} else if (score > 80.0) {
return 'B';
} else if (score > 70.0) {
return 'C';
} else if (score > 60.0) {
return 'D';
} else {
return 'F';
}
}
public static void main(String[] args) {
Scanner input = new Scanner(System.in);
System.out.print("Enter your score: ");
double score = input.nextDouble();
char grade = calculateGrade(score);
System.out.println("Your grade is: " + grade);
}
}
Bug? Yes! The boundary values
return incorrect results
To diagnose the issue using statistical debugging,
you could run the program with different inputs and
collect data about the variable score and the result
of the calculation.
You could also run the program multiple times with
the same inputs and collect data about the variables.
The Overall Approach
● Guess which behaviors are “potentially interesting” ○ Compile-time instrumentation of program
● Collect sparse, fair subset of these behaviors o Generic sampling framework
o Feedback profile + outcome label (success vs. failure) for each run
● Analyze behavioral changes in successful versus failing runs to find
bugs ○ Statistical debugging
82
Overall Architecture
Guesses
Top bugs
with likely
causes
Guesses
Statistical
Debugging
Program
Source
Profile +
 /

Deployed
Application Sampler
Compiler
83
A Model of Program Behaviors
● Assume any interesting behavior is expressible as a predicate
P
on program state at particular program point
Observation of program behavior = observing
P
• Example: to detect when a program enters an infinite loop, we might
define a predicate that checks whether a particular condition (such as
a loop counter exceeding a certain value) is true at a particular point in
the program's execution. • We could then evaluate this predicate at regular intervals during the
program's execution to determine whether the program is stuck in an
infinite loop.
● Instrument the program to observe each predicate
● A Question: Which predicates should we observe?
84
Branches Are Interesting
++branch_17[p != 0];
if (p) ...
else ...
63 p == 0
0
branch_17:
Track predicates:
p != 0
85
Return Values Are Interesting
n = fopen(...);
++call_41[(n==0)+(n>=0)];
23
call_41:
n < 0
n > 0
n == 0
Track predicates:
0
90
86
What Other Behaviors Are Interesting?
● Depends on the problem you wish to solve!
● Examples:
o Number of times each loop runs
o Scalar relationships between variables (e.g. i < j
, i > 42
)
o Pointer relationships (e.g. p == q
, p ! = null
)
o Memory usage and allocation patterns
o Function call relationships
o I/O operations and file system access
o User interactions
o Security related: authentication attempts, authorization failures,
or data access violations
87
QUIZ: Identify the Predicates
List all predicates tracked for this program, assuming only branches
are potentially interesting:
void main() {
int z;
for (int i = 0; i < 3; i++) {
char c = getc();
if (c == ‘a’)
z = 0;
else
z = 1;
assert(z == 1);
}
}
88
QUIZ: Identify the Predicates
List all predicates tracked for this program, assuming only branches
are potentially interesting:
c ==
‘
a
’ c != ‘a’
i < 3 i >= 3
void main() {
int z;
for (int i = 0; i < 3; i++) {
char c = getc();
if (c == ‘a’)
z = 0;
else
z = 1;
assert(z == 1);
}
}
89
Summarization and Reporting
63 p == 0
p != 0
0
branch_17:
23
call_41:
n < 0
n > 0
n == 0
0
90
p == 0
p != 0
n < 0
n > 0
n == 0
630
230
90
90
This summarization can provide a more complete
picture of program behavior by capturing data from
multiple sources and aggregating it into a single profile
that can be analyzed and reported on.
Summarization and Reporting
● Feedback report per run is:
○ Vector of predicate states ‒ , 0 , 1 , *
○ Success/failure outcome
label
● No time dimension, for
good or ill
outcome S / F
p == 0
p != 0
n < 0
n > 0
n == 0
630
230
90
1
0
91
QUIZ: Abstracting Predicate Counts
p == 0
p != 0
n < 0
n > 0
n == 0
630
230
90
1
0
‒ Never observed
0 False at least once, never true
1 True at least once, never false
* Both true and false at least once
92
QUIZ: Abstracting Predicate Counts
p == 0
p != 0
n < 0
n > 0
n == 0
630
230
90
1
0
*
0
*
‒ Never observed
0 False at least once, never true
1 True at least once, never false
* Both true and false at least once
93
QUIZ: Populate the Predicates
void main() {
int z;
for (int i = 0; i < 3; i++) {
char c = getc();
if (c ==
‘
a
’
)
z = 0;
else
z = 1;
assert(z == 1);
}
}
“bba” “bbb
”
c == ‘a’
c != ‘a’
i < 3
i >= 3
Outcome label (S/F)
Populate the predicate vectors and outcome labels for the two runs:
94
QUIZ: Populate the Predicates “bba” “bbb
”
c ==
‘
a
’ * 0
c != ‘a’ * 1
i < 3 1 *
i >= 3 0 *
Outcome label (S/F) F S
void main() {
int z;
for (int i = 0; i < 3; i++) {
char c = getc();
if (c == ‘a’)
z = 0;
else
z = 1;
assert(z == 1);
}
}
Populate the predicate vectors and outcome labels for the two runs:
95
The Need for Sampling
● Tracking all predicates is expensive
● Decide to examine or ignore each instrumented site: ○ Randomly
○ Independently
○ Dynamically
● Why? ○ Fairness
○ We need an accurate picture of rare events
○ Trade-offs between accuracy, fairness, and resource constraints.
p == 0
p != 0
n < 0
n > 0
n == 0
1
0
*
0
*
96
A Naive Sampling Approach
● Toss a coin at each instrumentation site
++count_42[p != NULL];
p = p->next;
++count_43[i < max];
total += sizes[i];
if (rand(100) == 0)
++count_42[p != NULL];
p = p->next;
if (rand(100) == 0)
++count_43[i < max];
total += sizes[i];
Too slow!
97
A dynamic sampling system must define a trigger mechanism that signals when a sample is to be taken.
sampling rate: 1%
Some Other Problematic Approaches
● Sample every kth site
○ Violates independence
○ Might miss predicates “out of phase”
● Periodic hardware timer or interrupt ○ Might miss rare events
○ Not portable
98
Amortized Coin Tossing
● Observation: Samples are rare (e.g. 1/100)
● Idea: Amortize sampling cost by predicting time until next sample
● Implement as countdown values selected from geometric distribution
● Models how many tails (0) before next head (1) for biased coin toss
● Example with sampling rate 1/5:
5 3
4
Next sample after:
sites
0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, …
99
An Efficient Approach
if (countdown >= 2) {
countdown -= 2;
p = p->next;
total += sizes[i];
} else {
if (countdown-- == 0) {
++count_42[p != NULL];
countdown = next();
}
p = p->next;
if (countdown-- == 0) {
++count_43[i < max];
countdown = next();
}
total += sizes[i];
}
if (rand(100) == 0)
++count_42[p !=NULL];
p = p->next;
if (rand(100) == 0)
++count_43[i < max];
total += sizes[i];
10
0
Feedback Reports with Sampling
● Feedback report per run is: ○ Vector of sampled predicate states (
‒
,
0
,
1
,
*
)
○ Success/failure outcome label
● Certain of what we did observe ○ But may miss some events
● Given enough runs, samples
≈ reality
○ Common events seen most often
○ Rare events seen at proportionate rate
10
1
QUIZ: Uncertainty Due to Sampling
Check all possible states that a predicate
P might take due to sampling.
The first column shows the actual state of
P (without sampling).
P
‒ 0 1
*
‒
0
1 ✅ ✅
*
10
2
QUIZ: Uncertainty Due to Sampling
Check all possible states that a predicate
P might take due to sampling.
The first column shows the actual state of
P (without sampling).
P
‒ 0 1
*
‒
✅
0 ✅ ✅
1 ✅ ✅
* ✅ ✅ ✅
✅
10
3
Overall Architecture Revisited
Guesses
Top bugs
with likely
causes
Guesses
Statistical
Debugging
Program
Source
Profile +
 /

Deployed
Application Sampler
Compiler
10
4
Finding Causes of Bugs
● We gather information about many predicates ○
≈ 300K for bc (“bench calculator” program on Unix)
● Most of these are not predictive of anything
● How do we find the few useful predicates?
10
5
Finding Causes of Bugs
How likely is failure when predicate P is observed to be true?
F(P) = # failing runs where
P is observed to be true
S(P) = # successful runs where
P is observed to be true
Example: F(P) = 20, S(P) = 30
⇒ Failure(P) = 20/50 = 0.4
Failure(P) = F(P)
F(P) + S(P)
10
6
Quiz
● Let’s say the predicate P is a > b
● We have a dataset of 100 runs of foo with random values for a and b.
● Out of these 100 runs, 60 were successful, and 40 failed due to an
assertion error. • F(P) = 30: Out of the 40 failing runs, in 30 of them, P (a > b) was observed to be true.
• S(P) = 45: Out of the 60 successful runs, in 45 of them, P (a > b) was observed to be true.
● What is the Failure(P) for the predicate P?
10
7
int foo(int a, int b) {
if (a > b) {
return a - b;
} else {
return b - a;
}
}
Quiz - Answer
● Let’s say the predicate P is a > b
● We have a dataset of 100 runs of foo with random values for a and b.
● Out of these 100 runs, 60 were successful, and 40 failed due to an
assertion error. • F(P) = 30: Out of the 40 failing runs, in 30 of them, P (a > b) was observed to be true.
• S(P) = 45: Out of the 60 successful runs, in 45 of them, P (a > b) was observed to be true.
● What is the Failure(P) for the predicate P?
10
8
int foo(int a, int b) {
if (a > b) {
return a - b;
} else {
return b - a;
}
}
Failure(P) = F(P) / (F(P) + S(P)) = 30 / (30 + 45) = 0.4
A higher Failure(P) value indicates a stronger association
between the predicate and the failure, suggesting that the
predicate is a more reliable predictor of failure.
Failure and the root-cause
● Whether a Failure(P) value is significant or not depends on the specific
application and the threshold set by the user or the organization. ○ In general, a higher Failure(P) value indicates a stronger association
between the predicate and the failure, suggesting that the predicate is a
more reliable predictor of failure.
○ However, in some cases, even a low Failure(P) value may be considered
significant if the predicate is critical or if there are no other reliable
predictors of failure.
○ On the other hand, a high Failure(P) value may not be significant if the
predicate is not relevant to the specific failure or if there are other more
reliable predictors of failure.
○ Therefore, the significance of the Failure(P) value should be assessed in
the context of the specific application and the knowledge of the domain
experts. 10
9
Tracking Failure Is Not Enough
Predicate x == 0 is an innocent predicate
● Program is already doomed
Failure(f == NULL) = 1.0
Failure(x == 0) = 1.0
if (f == NULL) {
x = foo();
*f;
}
int foo() {
return 0;
}
11
0
Therefore, we need a different statistic that takes into account the context in which
predicates are observed so that we don’t falsely consider predicates as being
predictors of program failure.
Tracking Context
How likely is failure when predicate P is observed to be true?
F(P observed) = # failing runs observing P
S(P observed) = # successful runs observing P
Example: F(P observed) = 40, S(P observed) = 80
⇒
Context(P) = 40/120 = 0.333…
Context(P) =
F(P observed)
F(P observed) + S(P observed)
11
1
Context measures the background chance of program failure regardless of the value of P
Quiz
● Predicate P : c > 15
● Out of 100 runs, the program fails in 10 runs. ○ Out of these 10 failed runs, we observe that the predicate P is observed
in 8 of them and is true in 4 of them.
○ In the remaining 90 successful runs, we observe the presence of
predicate P in 60 of them and is true in 20 of them.
● Calculate the Failure(P) and Context(P)?
11
2
int main() {
int a = 10;
int b = 5;
int c = a + b;
if (c > 15) {
// do something
}
return 0;
}
Quiz - Answer
● Predicate P : c > 15
● Out of 100 runs, the program fails in 10 runs. ○ Out of these 10 failed runs, we observe that the predicate P is observed
in 8 of them and is true in 4 of them.
○ In the remaining 90 successful runs, we observe the presence of
predicate P in 60 of them and is true in 20 of them.
● Calculate the Failure(P) and Context(P)?
Failure (P) = F(P=true)/ F(P=true) + S(P=true)
4/4 + 20 = 0.16
Context(P) = F(P observed)/ F(P observed) + S(P observed)
8/8+60 = 0.11
11
3
int main() {
int a = 10;
int b = 5;
int c = a + b;
if (c > 15) {
// do something
}
return 0;
}
A Useful Measure: Increase()
Does P being true increase chance of failure over the background rate?
Increase(P) = Failure(P) - Context(P)
● A form of likelihood ratio testing
● Increase(P)
≈
1
⇒ High correlation with failing runs
● Increase(P)
≈ -1
⇒ High correlation with successful runs
11
4
Quiz
● Let’s say P is an invariant. What is the Increase(p)?
11
5
Quiz - Answer
● Let’s say P is an invariant. What is the Increase(p)?
● If P is an invariant then P is true for all runs of a program, then
Increase(P) would equal zero! ○ Failure(P) and Context(P) will have the same value!
○ This means P has no correlation with the success or failure of a run.
● We assume that it were perfectly sampled; in practice, it will likely be
very close to but not exactly zero.
11
6
1 failing run:
Failure (f == NULL) = 1.00
Context (f == NULL) = 0.33
Increase(f == NULL) = 0.67
Failure (x == 0) = 1.00
Context (x == 0) = 1.00
Increase(x == 0) = 0.00
f == NULL
2 successful runs: f != NULL
Increase() Works
if (f == NULL) {
x = foo();
*f;
}
int foo() {
return 0;
}
11
7
When x is 0, there is little -- if any -- change in the probability of run failure. So the predicate x
== 0 is not “blamed” for causing the program to fail.
QUIZ: Computing Increase() “bba” “bbb” Failure Context Increase
c == ‘a’
*
0
c != ‘a’
* 1 0.5 0.5 0.0
i < 3
1
*
i >= 3
0
*
Outcome label
(S/F)
F S
11
8
QUIZ: Computing Increase() “bba” “bbb” Failure Context Increase
c == ‘a’
* 0 1.0 0.5 0.5
c != ‘a’
* 1 0.5 0.5 0.0
i < 3
1
* 0.5 0.5 0.0
i >= 3
0
* 0.0 0.5 -0.5
Outcome label
(S/F)
F S
11
9
Isolating the Bug
void main() {
int z;
for (int i = 0; i < 3; i++) {
char c = getc();
if (c == ‘a’)
z = 0;
else
z = 1;
assert(z == 1);
}
}
Increase
c == ‘a’ 0.5
c != ‘a’ 0.0
i < 3 0.0
i >= 3 -0.5
12
0
A First Algorithm
1. Discard predicates having Increase(P)
≤
0
○ e.g. bystander predicates, predicates correlated with success
○ Exact value is sensitive to few observations
○ Use lower bound of 95% confidence interval
2. Sort remaining predicates by Increase(P)
○ Again, use 95% lower bound
○ Likely causes with determinacy metrics
12
1
Isolating the Bug
Increase
c == ‘a’ 0.5
c != ‘a’ 0.0
i < 3 0.0
i >= 3 -0.5
void main() {
int z;
for (int i = 0; i < 3; i++) {
char c = getc();
if (c == ‘a’)
z = 0;
else
z = 1;
assert(z == 1);
}
}
12
2
Isolating a Single Bug in bc
void more_arrays()
{
...
/* Copy the old arrays. */
for (indx = 1; indx < old_count; indx++)
arrays[indx] = old_ary[indx];
/* Initialize the new elements. */
for (; indx < v_count; indx++)
arrays[indx] = NULL;
...
}
#1: indx > scale
#2: indx > use_math
#3: indx > opterr
#4: indx > next_func
#5: indx > i_base
for (; indx < a_count; indx++)
12
3
It Works!
● At least for programs with a single bug
● Real programs typically have multiple, unknown bugs
● Redundancy in the predicate list is a major problem
12
4
Using the Information
● Multiple useful metrics: Increase(P)
, Failure(P)
, F(P)
, S(P)
● Organize all metrics in compact visual (bug thermometer)
12
5
Sample Report
12
6
Multiple Bugs: The Goal
Find the best predictor for each bug,
without prior knowledge of the number of bugs,
sorted by the importance of the bugs.
12
7
Multiple Bugs: Some Issues
● A bug may have many redundant predictors ○ Only need one
○ But would like to know correlated predictors
● Bugs occur on vastly different scales ○ Predictors for common bugs may dominate, hiding predictors
of less common problems
12
8
An Idea
● Simulate the way humans fix bugs
● Find the first (most important) bug
● Fix it, and repeat
12
9
Revised Algorithm
Repeat
Step 1: Compute Increase(), F(), etc. for all predicates
Step 2: Rank the predicates
Step 3: Add the top-ranked predicate
P to the result list
Step 4: Remove
P and discard all runs where
P is true
■ Simulates fixing the bug corresponding to
P
■ Discard reduces rank of correlated predicates
Until no runs are left
13
0
Ranking by Increase(P)
Problem: High Increase() scores but few failing runs!
Sub-bug predictors: covering special cases of more general bugs
13
1
Ranking by F(P)
Problem: Many failing runs but low Increase() scores!
Super-bug predictors: covering several different bugs together
13
2
A Helpful Analogy
● In the language of information retrieval ○ Precision = fraction of retrieved instances that are relevant
○ Recall = fraction of relevant instances that are retrieved
● In our setting: ○ Retrieved instances ~ predicates reported as bug predictors
○ Relevant instances ~ predicates that are real bug predictors
● Trivial to achieve only high precision or only high recall
● Need both high precision and high recall
13
3
Combining Precision and Recall
● Increase(P) has high precision, low recall
● F(P) has a high recall, low precision
● Standard solution: take the harmonic mean of both
● Rewards high scores in both dimensions 2 1/Increase(P) + 1/F(P)
=
13
4
Sorting by the Harmonic Mean
It works!
13
5
What Have We Learned?
● Monitoring deployed code to find bugs
● Observing predicates as model of program behavior
● Sampling instrumentation framework
● Metrics to rank predicates by importance: Failure(P)
, Context(P)
,
Increase(P), ...
● Statistical debugging algorithm to isolate bugs
13
6
Key Takeaways (1 of 2)
● A lot can be learned from actual executions o Users are executing them anyway
o We should capture some of that information
13
7
Key Takeaways (2 of 2)
● Crash reporting is a step in the right direction o But stack is useful for only about 50% of bugs
o Doesn’t characterize successful runs ■ But this is changing ...
13
8
References ● Some slides are taken from the following links: ○ https://software-analysis-class.org/
○ Lecture slides from topic “Instrumentation of Software Systems” by Dr.James H.Hill, Indiana
university-Purdue University Indianapolis
● Additional Reading: ○ https://www.debuggingbook.org/html/StatisticalDebugger.html
○ Paper: “Dapper, a Large
-Scale Distributed Systems Tracing Infrastructure”
○ Chapter 2,3,4 from “Distributed Tracing in Practice Instrumenting, Analyzing, and Debugging
Microservices”
13
9
6. Dataflow Analysis
COSC 3P95:
Software Testing & Analysis
Winter 2023
1
Motivation
● Static analysis reasoning about flow of data in programs
● Different kinds of data: constants, variables, expressions
● Used by bug-finding tools and compilers
x = 5;
y = 1;
while (x != 1) {
y = x * y;
x = x - 1
}
The WHILE Language
(statement)
S ::= x = a | S1 ; S2 |
if (b) { S1 } else { S2 } | while (b) { S1 }
(arithmetic expression)
a ::=
x | n | a1 + a2 | a1 - a2 | a1 * a2
(boolean expression)
b ::= true | ! b | b1 && b2 | a1 != a2
(integer variable)
x
(integer constant)
n
Control-Flow Graphs
x = 5
(x != 1)?
exit y = x * y
entry
y = 1
x = x - 1
true false
x = 5;
y = 1;
while (x != 1) {
y = x * y;
x = x - 1
}
Basic Block
QUIZ: Control-Flow Graphs
entry
x = 5
(x != 0)?
false true
false
exit
x = x - 1
y = y - 1
y = x
(y != 0)?
true
x = 5;
while (x != 0) {
y = x;
x = x - 1;
while (y != 0) {
y = y - 1
}
}
QUIZ: Control-Flow Graphs
entry
x = 5
(x != 0)?
false true
false
exit
x = x - 1
y = y - 1
y = x
(y != 0)?
true
Soundness, Completeness, Termination
● Impossible for any analysis to achieve all three together
● Dataflow analysis sacrifices completeness
● Sound: Will report all facts that could occur in actual runs
● Incomplete: May report additional facts that can’t occur
in actual runs
Abstracting Control-Flow Conditions
● Abstracts away control-flow conditions
with non-deterministic choice (
*
)
● Non-deterministic choice => assumes
condition can evaluate to true or false
● Considers all paths possible in actual runs
(sound), and maybe paths that are never
possible (incomplete
)
x = 5
(x != 1)?
exit y = x * y
entry
y = 1
x = x - 1
true false
*
Classic Dataflow Analyses
Very Busy Expressions Analysis
● Reduce code size
Live Variables Analysis
● Allocate registers efficiently
Reaching Definitions Analysis ● Find uninitialized variable uses
Available Expressions Analysis ● Avoid recomputing expressions
Modern Dataflow Analyses
Type-State Analysis
● Temporal safety properties
(APIs of protocols, libraries,
…
)
Concurrency Analysis
● Concurrency safety properties
(dataraces, deadlocks,
…
)
Interval Analysis ● Check memory safety
(integer overflows, buffer overruns,
…
)
Taint Analysis ● Check information flow
(Sensitive data leak, code injection,
…
)
1. Reaching Definitions Analysis
Goal: Determine, for each program
point,
which assignments have been
made
and not overwritten, when
execution reaches that point
along
some path
P2
x = 5
(x != 1)?
exit y = x * y
entry
y = 1
x = x - 1
true false
● “Assignment” == “Definition”
P1
QUIZ: Reaching Definitions Analysis
1. The assignment y = 1 reaches P1
2. The assignment y = 1 reaches P2
3. The assignment y = x * y reaches P1
x = 5
(x != 1)?
exit y = x * y
entry
y = 1
x = x - 1
true false
P1
P2
QUIZ: Reaching Definitions Analysis
1. The assignment y = 1 reaches P1
2. The assignment y = 1 reaches P2
3. The assignment y = x * y reaches P1
x = 5
(x != 1)?
exit y = x * y
entry
y = 1
x = x - 1
true false
P1
P2
Result of Dataflow Analysis (Informally)
● Set of facts at each program point
● For reaching definitions analysis,
fact is a pair of the form:
<defined variable name, defining
node label>
● Examples: <x,2>
, <y,5>
1:
2:
3:
4:
7: 5:
6:
x = 5
(x != 1)?
exit y = x * y
entry
y = 1
x = x - 1
true false
Result of Dataflow Analysis (Formally)
● Give a distinct label
n to each node
● IN[n] = set of facts at entry of node
n
● OUT[n] = set of facts at exit of node
n
● Dataflow analysis computes IN[n] and OUT[n]
for each node
● Repeat two operations until IN[n] and OUT[n]
stop changing ○ Called “saturated” or “fixed point
”
1:
2:
3:
4:
7: 5:
6:
x = 5
(x != 1)?
exit y = x * y
entry
y = 1
x = x - 1
true false
Reaching Definitions Analysis: Operation #1
n’
∈
predecessors(n)
IN[n] = OUT[n’]
n
n1 n2
∪ n3
IN[n] = OUT[n1]
∪ OUT[n2]
∪ OUT[n3]
Reaching Definitions Analysis: Operation #2
n: GEN[n]
= KILL[n]
=
n:
n:
GEN[n] = { <x, n> }
KILL[n] = { <x, m> : m != n }
OUT[n] = (IN[n] - KILL[n]
)
∪ GEN[n]
x = a
b ?
OUT[n]
IN[n]
∅
∅
Overall Algorithm: Chaotic Iteration
n’
∈ predecessors(n)
for (each node n):
IN[n] = OUT[n] =
OUT[entry] = { <v, ?> : v is a program variable }
repeat:
for (each node n):
IN[n] = OUT[n’]
OUT[n] = (IN[n] - KILL[n]
)
∪ GEN[n]
until IN[n] and OUT[n] stop changing for all n ∪
∅
Does It Always Terminate?
The Chaotic Iteration algorithm always terminates!
● The two operations of reaching definitions analysis are “monotonic
”
=> IN and OUT sets never shrink, only grow
● Largest they can be is set of all definitions in program, which is finite
=> IN and OUT cannot grow forever
=> IN and OUT will stop changing after some iteration
Reaching Definitions Abstract Domain
● Any combination of the definitions
<x, 2>
, <y,3>
, <y,5>
, <y,6> may reach
a particular program point
● So, each combination of definitions
is an abstract value
● Abstract domain is: ⟨
,
⊆
⟩
1:
2:
3:
4:
7: 5:
6:
x = 5
(x != 1)?
exit y = x * y
entry
y = 1
x = x - 1
true false
set inclusion
2 { <x,2>, <y,3>, <y,5>, <y,6> }
Reaching Definitions Abstract Domain
{<x,2>,<y,3>} {<y,5>,<x,6>}
{<x,2>,<y,3>,<y,5>}
{<x,2>,<y,3>,<y,5>,<x,6>}
{<y,3>,<y,5>,<x,6>} {<x,2>,<y,5>,<x,6>} {<x,2>,<y,3>,<x,6>}
{<x,2>,<y,5>} {<y,3>,<y,5>}
{<x,2>} {<y,3>} {<y,5>}
{<x,2>,<x,6>}
∅
{<x,6>}
{<y,3>,<x,6>}
incomparable
in precision Most precise
Meast precise
lower is
more precise
n IN[n] OUT[n]
1 -- {<x,?>,<y,?>}
2
3
4
5
6
7 --
Reaching Definitions Analysis Example ∅
∅
∅
∅
∅
∅
∅
∅
∅
∅
∅
1:
2:
3:
4:
7: 5:
6:
x = 5
(x != 1)?
exit y = x * y
entry
y = 1
x = x - 1
true false
Reaching Definitions Analysis Example
n IN[n] OUT[n]
1 -- {<x,?>,<y,?>}
2 {<x,?>,<y,?>} {<x,2>,<y,?>}
3 {<x,2>,<y,?>} {<x,2>,<y,3>}
4
5
6
7 --
∅
∅
∅
∅
∅
∅
∅
1:
2:
3:
4:
7: 5:
6:
x = 5
(x != 1)?
exit y = x * y
entry
y = 1
x = x - 1
true false
QUIZ: Reaching Definitions Analysis
n IN[n] OUT[n]
1 -- {<x,?>,<y,?>}
2 {<x,?>,<y,?>} {<x,2>,<y,?>}
3 {<x,2>,<y,?>} {<x,2>,<y,3>}
4
5
6
7 --
1:
2:
3:
4:
7: 5:
6:
x = 5
(x != 1)?
exit y = x * y
entry
y = 1
x = x - 1
true false
QUIZ: Reaching Definitions Analysis
n IN[n] OUT[n]
1 -- {<x,?>,<y,?>}
2 {<x,?>,<y,?>} {<x,2>,<y,?>}
3 {<x,2>,<y,?>} {<x,2>,<y,3>}
4 {<x,2>,<y,3>,<y,5>,<x,6>} {<x,2>,<y,3>,<y,5>,<x,6>}
5 {<x,2>,<y,3>,<y,5>,<x,6>} {<x,2>,<y,5>,<x,6>}
6 {<x,2>,<y,5>,<x,6>} {<y,5>,<x,6>}
7 {<x,2>,<y,3>,<y,5>,<x,6>} --
1:
2:
3:
4:
7: 5:
6:
x = 5
(x != 1)?
exit y = x * y
entry
y = 1
x = x - 1
true false
2. Very Busy Expressions Analysis
Goal: Determine very busy expressions
at each program point
P
An expression is very busy if, no
matter what path is taken, the
expression is used before any of
the variables occurring in it are
redefined
(a != b)?
entry
true false
a = 0
y = b - a x = b - a
y = a - b x = a - b
exit
b - a is very busy at point P
a - b is not very busy at P
Very Busy Expressions Analysis: Operation #1
OUT[n] = IN[n1]
⋂ IN[n2]
⋂ IN[n3]
n
n1 n2 n3 n’
∈
successors(n)
OUT[n] =
∪IN[n’]
Very Busy Expressions Analysis: Operation #2
n:
n:
n:
GEN[n] = { a }
KILL[n] = { expression e : e contains x }
IN[n] = (OUT[n] - KILL[n]
)
∪ GEN[n]
x = a
b ?
IN[n]
OUT[n]
GEN[n]
= KILL[n]
=
∅
∅
The set of expressions at the entry of node n is equal to the set of expressions at the
exit of node n, minus any expressions using variables that are modified by node n,
unioned with any new expressions that are used by node n.
Overall Algorithm: Chaotic Iteration
for (each node n):
IN[n] = OUT[n] = set of all expressions in program
IN[exit] =
repeat:
for (each node n):
OUT[n] = IN[n’]
IN[n] = (OUT[n] - KILL[n]
)
∪ GEN[n]
until IN[n] and OUT[n] stop changing for all n ∅ n’
∈ successors(n) ∪
Very Busy Expressions Abstract Domain
● Expressions a-b
, b-a may independently be
“very busy” at a particular program point
● So, each combination of these expressions
is an abstract value
● Abstract domain is:
a = 0
y = b - a
(a != b)?
exit
x = b - a
entry
y = a - b x = a - b
true false
reverse set inclusion
2 { b-a, a-b }
⟨
,
⊇
⟩
These abstract values are partially ordered by reverse set inclusion, since a larger
set of very busy expressions corresponds to a more precise abstract value.
Very Busy Expressions Abstract Domain
{a-b, b-a}
{b-a} {a-b}
a = 0
y = b - a
(a != b)?
exit
x = b - a
entry
y = a - b x = a - b
true false
∅ Least precise
Most precise
Very Busy Expressions Analysis Example
n IN[n] OUT[n]
1 -- { b-a, a-b }
2 { b-a, a-b } { b-a, a-b }
3 { b-a, a-b } { b-a, a-b }
4 { b-a, a-b } { b-a, a-b }
5 { b-a, a-b } { b-a, a-b }
6 { b-a, a-b } { b-a, a-b }
7 { b-a, a-b } { b-a, a-b }
8 --
1:
2: (a != b)?
entry
true false
∅
3:
6:
4:
5:
7:
8:
a = 0
y = b - a x = b - a
y = a - b x = a - b
exit
Very Busy Expressions Analysis Example
n IN[n] OUT[n]
1 -- { b-a, a-b }
2 { b-a, a-b } { b-a, a-b }
3 { b-a, a-b } { b-a, a-b }
4 { b-a, a-b } { b-a, a-b }
5 { b-a, a-b } { b-a, a-b }
6 { a-b }
7 { a-b }
8 --
1:
2:
3:
6:
4:
5:
7:
8:
a = 0
y = b - a
(a != b)?
x = b - a
entry
y = a - b x = a - b
true false
∅
∅
∅
exit
QUIZ: Very Busy Expressions Analysis
1:
2:
3:
6:
4:
5:
7:
8:
a = 0
y = b - a
(a != b)?
x = b - a
entry
y = a - b x = a - b
true false
n IN[n] OUT[n]
1 --
2
3
4
5 { a-b }
6 { a-b }
7 { a-b }
8 --
∅
∅
∅
∅
exit
QUIZ: Very Busy Expressions Analysis
1:
2:
3:
6:
4:
5:
7:
8:
a = 0
y = b - a
(a != b)?
x = b - a
entry
y = a - b x = a - b
true false
n IN[n] OUT[n]
1 -- { b-a }
2 { b-a } { b-a }
3 { b-a, a-b } { a-b }
4 { b-a }
5 { a-b }
6 { a-b }
7 { a-b }
8 --
∅
∅
∅
∅
∅
exit
3. Available Expressions Analysis
Goal: Determine, for each program point,
which expressions whose results are already
available,
and
not later modified, on all paths to the point.
P
x = a - b
y = a * b
(y != a - b)?
exit
x = a - b
entry
a = a - 1
true false
Used in debugging and code optimizations, such as
common subexpression elimination and partial
redundancy elimination.
a
– b is available at P
Example 2: Available Expressions
0 m = 5
1 n = 2
2 result = 0
3 h = m * n
4 while (result != m *n) {
5 b = (i * 2) + 3
6 c = m * n
7 result += c
8 e = m * n
9 result += e
–
b
10
}
37
Available Expressions:
At point P (right before line
#4 and before #6 or
before #8):
m * n is available (computed
before and its elements
never changed after)
So, m * n is available in the
whole loop and can be
computed once and
reused later.
…
3 h = m * n
4 while (result !=
h) {
5 b = (i * 2) + 3
6 c =
h
7 result += c
8 e =
h
9 result += e
–
b
10 }
Example 3: Very Busy Expressions
1 m = 5
2 n = 2
3 result = 0
4 while (result != m *n){
5 b = (i * 2) + 3
6 c = m * n
7 result += c
8 e = m * n
9 result += e
–
b
10
}
38
Very Busy Expressions:
At point P (right before line
#4):
m * n is very busy (will never
get changed regardless of
path taken)
…
3 temp = m * n
4 while (result != temp){
5 b = (i * 2) + 3
6 c = temp
7 result += c
8 e = temp
9 result += e - b
10
}
Available Expressions Abstract Domain
● Expressions a-b
, a*b
, a-1 may independently
be “available” at a particular program point
● So, each combination of these expressions
is an abstract value
● Define lattice as:
reverse set inclusion
x = a - b
y = a * b
(y != a - b)?
exit
x = a - b
entry
a = a - 1
true false
2 { a-b, a*b, a-1 }
⟨
,
⊇
⟩
A larger set of available expressions corresponds to a more precise abstract value
Available Expressions Abstract Domain
{a-b, a*b}
{a-1}
{a*b, a-1}
{a-b}
{a-b, a*b, a-1}
{a*b}
{a-b, a-1} ∅ x = a - b y = a * b (y != a - b)?
exit
x = a - b
entry
a = a - 1
true false
most precise
least precise
Algorithm: The IN set will be any expressions which have been calculated earlier in the code without having the
variables in their calculations overwritten. Our OUT set will be our IN set minus any expressions which have a
variable that is overwritten by that statement, plus any expressions that are generated by that statement.
Available Expressions Analysis Example
n IN[n] OUT[n]
1 --
2 {a-b, a*b, a-1} {a-b, a*b, a-1}
3 {a-b, a*b, a-1} {a-b, a*b, a-1}
4 {a-b, a*b, a-1} {a-b, a*b, a-1}
5 {a-b, a*b, a-1} {a-b, a*b, a-1}
6 {a-b, a*b, a-1} {a-b, a*b, a-1}
7 {a-b, a*b, a-1} --∅
1:
2:
3:
4:
5:
6:
7:
x = a - b
y = a * b
(y != a - b)?
exit
x = a - b
entry
a = a - 1
true false
Algorithm: The IN set will be any expressions which have been calculated earlier in the code without having the
variables in their calculations overwritten. Our OUT set will be our IN set minus any expressions which have a
variable that is overwritten by that statement, plus any expressions that are generated by that statement.
Available Expressions Analysis Example
n IN[n] OUT[n]
1 --
2 {a-b}
3 {a-b} {a-b, a*b}
4 {a-b, a*b} {a-b, a*b}
5 {a-b, a*b}
6 {a-b}
7 {a-b, a*b} --∅
∅
∅
∅
1:
2:
3:
4:
5:
6:
7:
x = a - b
y = a * b
(y != a - b)?
exit
x = a - b
entry
a = a - 1
true false
Available Expressions Analysis Example
n IN[n] OUT[n]
1 --
2 {a-b}
3 {a-b} {a-b, a*b}
4 {a-b} {a-b}
5 {a-b}
6 {a-b}
7 {a-b} --∅
∅
∅
∅
1:
2:
3:
4:
5:
6:
7:
x = a - b
y = a * b
(y != a - b)?
exit
x = a - b
entry
a = a - 1
true false
4. Live Variables Analysis
Goal: Determine, for each program point,
which variables could be live at
that point.
A variable is live if there is a path
to a use of the variable that does
not redefine the variable.
P y = 4
x = 2
(y != x)?
entry
true false
z = y z = y * y
exit
x = z
variable
y is available at P
Used in optimizations such as removing unnecessary
variable assignments or dead code.
Live Variables Abstract Domain
● Variables
x
,
y
,
z may independently be
live at a particular program point
● So, each combination of these variables
is an abstract value
● Abstract domain is:
y = 4
x = 2
(y != x)?
entry
true false
z = y z = y * y
exit
x = z
set inclusion
2 { x, y, z }
⟨
,
⊆
⟩
A smaller set of live variables corresponds to a more precise
abstract value. Because a smaller set of live variables means that
the analysis can eliminate more variables that are not live at that
point, resulting in a more precise abstract value.
This is similar to what we saw for reaching definitions analysis.
variable
y is available at PP
Live Variables Abstract Domain
y = 4
x = 2
(y != x)?
entry
true false
z = y z = y * y
exit
x = z
{x,y,z}
{x,z}
{x} {z} {y}∅
{x,y} {y,z}
Algorithm: the IN and OUT sets of each node in the control-flow graph start out with the bottom abstract value, the
empty set of variables. In each iteration, these sets grow progressively, along the edges in this structure. The most
that any IN or OUT set can grow is the top abstract value, the set of all variables in the program.
Most precise
Least precise
Live Variables Analysis Example
n IN[n] OUT[n]
1 --
2
3
4
5
6
7
8 -- ∅
∅
∅
∅
∅
∅
∅
∅
∅
∅
∅
∅
∅
∅
y = 4
x = 2
(y != x)?
entry
true false
z = y z = y * y
exit
x = z
1:
2:
3:
4:
5:
7:
6:
8:
Algorithm: the IN and OUT sets of each node in the control-flow graph start out with the bottom abstract value, the
empty set of variables. In each iteration, these sets grow progressively, along the edges in this structure. The most
that any IN or OUT set can grow is the top abstract value, the set of all variables in the program.
Live Variables Analysis Example
n IN[n] OUT[n]
1 --
2 { y }
3 { y } { x, y }
4 { x, y } { y }
5 { y } { z }
6 { y } { z }
7 { z }
8 -- ∅
∅
∅
∅
y = 4
x = 2
(y != x)?
entry
true false
z = y z = y * y
exit
x = z
1:
2:
3:
4:
5:
7:
6:
8:
Backward algorithm and we will start at the exit point (#8)!
Live Variables Analysis Example
n IN[n] OUT[n]
1 --
2 { y }
3 { y } { x, y }
4 { x, y } { y }
5 { y } { z }
6 { y } { z }
7 { z }
8 -- ∅
∅
∅
∅
Backward algorithm and we will start at the exit point (#8)!
• Node 7 OUT set is the same as
Node 8 IN set (OUT[7])
• For IN set: use OUT set, kill
redefined variables, add
variables used in the node
• IN set of Node 7 is {z}
• Both Nodes 5 and 6 have IN set
of Node 7 as their OUT set ({z}
in OUT[5] and OUT[6])
• Node 6 redefines z, uses y, so
remove z and add y to IN set ({y}
in IN[6])
• Node 5 redefines z, uses y, so
remove z and add y to IN set ({y}
in IN[5])
• Node 4 uses both y and x
without redefining them, so its IN
set is {x,y}
• Node 3 kills x in between its
OUT and IN. Node 2 kills y
y = 4
x = 2
(y != x)?
entry
true false
z = y z = y * y
exit
x = z
1:
2:
3:
4:
5:
7:
6:
8:
Overall Pattern of Dataflow Analyses
[n] = ( [n] - KILL[n])
∪ GEN[n]
n’
∈ (n)
[n] = [n’]
= IN or OUT
= U (may) or
= predecessors or successors ∩ (must)
The blue and red boxes represent the IN or OUT sets.
The purple box represents the set union or set intersection operator.
The black box represents the immediate predecessors or immediate successors of a node.
Reaching Definitions Analysis
[n] = ( [n] - KILL[n])
∪ GEN[n]
n’
∈ (n)
[n] = [n’]
= IN or OUT
= U (may) or
= predecessors or successors ∩ (must)
OUT
OUT IN
IN
preds U
Propagates forward in the control-flow graph
Computes “may” information, because the goal is to find definitions that could reach a program point
along some path
Very Busy Expressions Analysis
[n] = ( [n] - KILL[n])
∪ GEN[n]
n’
∈ (n)
[n] = [n’]
= IN or OUT
= U (may) or
= predecessors or successors ∩ (must)
IN
IN OUT
OUT
succs ∩
Propagates backward in the control-flow graph
Computes “must
” information, because the goal is to find expressions that are used along all paths
before any variable occurring in them is redefined
QUIZ: Available Expressions Analysis
[n] = ( [n] - KILL[n])
∪ GEN[n]
n’
∈ (n)
[n] = [n’]
= IN or OUT
= U (may) or
= predecessors or successors ∩ (must)
QUIZ: Available Expressions Analysis
[n] = ( [n] - KILL[n])
∪ GEN[n]
n’
∈ (n)
[n] = [n’]
= IN or OUT
= U (may) or
= predecessors or successors ∩ (must)
OUT
OUT IN
IN
preds ∩
Propagates forward in the control-flow graph
Computes “must
” information, because the goal is to find expressions that are available at a program
point, meaning that must have been computed along all paths leading to a point.
QUIZ: Live Variables Analysis
[n] = ( [n] - KILL[n])
∪ GEN[n]
n’
∈ (n)
[n] = [n’]
= IN or OUT
= U (may) or
= predecessors or successors ∩ (must)
QUIZ: Live Variables Analysis
[n] = ( [n] - KILL[n])
∪ GEN[n]
n’
∈ (n)
[n] = [n’]
= IN or OUT
= predecessors or successors
IN
IN OUT
OUT
succs U
= U (may) or
∩ (must)
Propagates backward in the control-flow graph
Computes “may
” information, because it seeks to find live variables: variables that may be used along
some path before being re-defined.
May Analysis vs Must Analysis
• "May analysis" is used to find things that might happen in a program,
but it's not guaranteed. For example, a program might use a certain
value in a calculation, but only if some condition is met. If we're not
sure whether the condition will be met, we say the value "may" be
used.
• "Must analysis" is used to find things that are guaranteed to happen in
a program. For example, a program always adds two numbers
together before showing the result. In this case, we know the addition
"must" happen every time the program runs.
57
Reaching
Definitions
QUIZ: Classifying Dataflow Analyses
Very Busy
Expressions
Live
Variables
Available
Expressions
Match each analysis with its characteristics.
May Must
Forward
Backward
QUIZ: Classifying Dataflow Analyses
May Must
Forward Reaching Definitions Available Expressions
Backward Live Variables Very Busy Expressions
Match each analysis with its characteristics.
Quiz
Which data flow analysis technique helps to determine which variables
hold values that may be used later in the program, allowing for
optimizations such as dead code elimination?
1- Reaching definitions analysis 2- Available expressions analysis
3- Live variables analysis 4- Very busy expressions analysis
60
Answer is option 3: Live Variables Analysis
Quiz
Given the following code, which data flow analysis
technique can be applied to optimize the code by
eliminating redundant calculations?
61
1- Reaching definitions analysis 2- Available expressions analysis
3- Live variables analysis 4- Very busy expressions analysis
int z;
int result;
int x = a * b;
int y = a + b;
if (a > b) {
z = x * y;
result = z + (a + b);
} else {
z = x * y;
result = z
– (a + b);
}
return result;
Answer is option 2: Available expressions analysis
Available Expressions Analysis can be applied to this
code to optimize it by eliminating the redundant
calculation of (a + b). Since y = a + b has already been
calculated, we can replace (a + b) with y in the
expressions result = z + (a + b) and result = z - (a + b)
to make the code more
